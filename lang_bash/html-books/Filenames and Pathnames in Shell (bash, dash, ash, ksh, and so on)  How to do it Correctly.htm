<!DOCTYPE html>
<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Filenames and Pathnames in Shell (bash, dash, ash, ksh, and so on): How to do it Correctly</title>
<meta name="description" content="This article explains how to correctly handle filenames in Bourne shells (the primary shell of Unix/Linux/POSIX systems), based on the issues discussed in 'Fixing Unix/Linux/Filenames'. Many programs fail to work properly on filenames that include spaces, begin with dash (-), include newlines, and so on, because developers don't know how to do it properly.  Many texts, even good ones, get this wrong.">
<meta name="keywords" content="Unix, Linux, POSIX, filename, file name, filenames, file names, control character, newline, tab, escape, leading dash, leading dashes, leading hyphen, UTF-8, encoding, metacharacter, limits, limitations, fixing, filesystem, newlines in filenames, newline in filename, control characters in filenames, leading hypen, filesystem, file system, space in filename, spaces in filenames, pathname, path name, pathnames, path names">
<meta name="generator" content="vim">
<link rel="stylesheet" type="text/css" href="Filenames%20and%20Pathnames%20in%20Shell%20%28bash,%20dash,%20ash,%20ksh,%20and%20so%20on%29%20%20How%20to%20do%20it%20Correctly_files/paper.css">

</head><body bgcolor="#FFFFFF">

<h1 class="title">Filenames and Pathnames in Shell: How to do it Correctly</h1>
<h2 class="author">David A. Wheeler</h2>
<h2 class="date">2014-11-03 (original version 2010-05-19)</h2>
<p>

</p><p>
Many Bourne shell scripts (as run by bash, dash, ash, ksh, and so on)
do <i><b>not</b></i> handle filenames and pathnames correctly on
Unix-like/POSIX systems.
Some shell programming books teach it wrongly, and even the
<a href="http://austingroupbugs.net/view.php?id=248">POSIX standard
sometimes gets it wrong</a>.
Thus, many shell scripts are buggy, leading to surprising failures
and in some cases security vulnerabilities (see the
<a href="http://www.dwheeler.com/secure-programs/Secure-Programs-HOWTO/file-names.html">“Secure Programming for Linux and Unix HOWTO” section on filenames</a>,
<a href="https://www.securecoding.cert.org/confluence/display/seccode/MSC09-C.+Character+Encoding+-+Use+Subset+of+ASCII+for+Safety">
CERT’s “Secure Coding” item MSC09-C</a>,
<a href="http://cwe.mitre.org/data/definitions/78.html">CWE 78</a>,
<a href="http://cwe.mitre.org/data/definitions/73.html">CWE 73</a>,
<a href="http://cwe.mitre.org/data/definitions/116.html">CWE 116</a>,
and the
<a href="http://cwe.mitre.org/top25/index.html">CWE/SANS
Top 25 Most Dangerous Programming Errors</a>).
This is a real problem, because on Unix-like systems
(e.g., Unix, Linux, or POSIX) shells are universally available and
widely used for lots of basic tasks.

</p><p>
This essay shows <a href="#wrong">common <i>wrong</i> ways</a>
to handle filenames and pathnames in Bourne shells, and gives a
<a href="#summary">summary of how to do it correctly for the impatient</a>.
It then walks through <a href="#basic-rationale">rationale</a>
so you can <i>understand</i> why common techniques do not work...
and why the alternatives do.
I presume that you already know how to write Bourne shell scripts.

</p><p>
The basic problem is that today
<a href="http://www.dwheeler.com/essays/fixing-unix-linux-filenames.html">most Unix-likes allow
filenames to include almost <i>any</i> bytes</a>.
That includes newlines, tabs,
the escape character
(including escape sequences that can execute commands when displayed),
other control characters,
spaces (anywhere!), leading dashes (-), shell metacharacters,
and byte sequences that aren’t legal UTF-8 strings.
So your scripts could be fail or even be subverted if
you ever unarchive “tar” or “zip” files from someone else,
examine directories with files created by someone else,
or simply create files yourself that contain shell metacharacters
(like space or question mark).

</p><p>
This is not a <i>just</i> a shell problem.
Lots of code in <i>all</i> languages (not just shell), and at least
some GUI toolkits, do not handle all permitted
filenames and pathnames correctly.
Some GUI toolkits (e.g., file-pickers)
presume that filenames are always in UTF-8 and
never contain control characters, even though neither are necessarily true. 

</p><p>
However, this
<a href="http://www.dwheeler.com/essays/fixing-unix-linux-filenames.html">flaw
in Unix-like kernels (allowing dangerous filenames)</a> combines
with additional weaknesses in the Bourne shell language,
making it even <i>more</i> difficult in shell to correctly handle
filenames and pathnames.
I think shell is a reasonable language for short scripts,
when properly used, but the excessive permissiveness of filenames
turns easy tasks into easily-done-wrong tasks.
A <a href="http://www.dwheeler.com/essays/fixing-unix-linux-filenames.html">few small changes would
make it much easier to write secure code for handling filenames</a>
for all languages including shell.
So if your script may handle unarchived files, or files created by
different user or mobile app, then
your script needs to handle this botched situation.
Tools like <a href="http://www.shellcheck.net/">shellcheck</a> can
help you find some of these problems, but not all of them, and
you can use such tools more effectively if you understand the problem.

</p><p>
First, though, some key terminology.
A <a href="http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap03.html#tag_03_267"><i>pathname</i> is used to identify a particular file</a>,
and may include zero or more “/” characters.
Each pathname component (separated by “/”) is a officially
called a
<a href="http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap03.html#tag_03_267">filename</a>;
pathname components (aka filenames) cannot contain “/”.
So officially “/usr/bin/sh” is a pathname, with
pathname components (filenames) inside it, that refers to a particular file.
(Note: on Cygwin, “\” is a synonym for “/”,
so it also separates pathname components.)
In practice, many people use the term “filename”
to mean both pathname components (which are officially filenames)
and entire pathnames.
Neither pathname components nor full pathnames can contain
the NUL character (\0), because that is the terminator, and
pathname components also cannot include “/”;
those turn out to be the only rules you can really count on today.

</p><p>
</p><h1 id="wrong">How to do it wrongly</h1>
<p>
First, let’s go through some examples that are <i>wrong</i>, because
the first step to fixing things is to know what’s broken.
These examples assume default settings
(e.g., there is no “set -f” or “IFS=...”):
</p><p>
<table border="1">
<tbody><tr><td>
<p>
</p><pre>cat * &gt; ../collection  # WRONG
</pre>
<blockquote>
This is wrong. If a filename in the current directory
begins with “-”, it will be misinterpreted as an
option instead of as a filename.
For example,
if there’s a file named “-n”, it will
suddenly enable cat’s “-n” option instead if it has one
(GNU cat does, it numbers the lines).
In general you should <b>never</b> have a glob
that begins with “*” —
it should be prefixed with “./”.
Also, if there are no (unhidden) files in the directory,
the glob pattern will return the pattern instead (“*”);
that means that the command (cat)
will try to open a file with the improbable name “*”.
</blockquote>
</td></tr>
<tr><td>
<p>
</p><pre>for file in * ; do  # WRONG
  cat "$file" &gt;&gt; ../collection
done
</pre>
<blockquote>
Also wrong, for the same reason; a file named
“-n” will fool the <tt>cat</tt> program, and if the pattern
does not match, it will loop once with the pattern itself as the value.
</blockquote>
</td></tr>
<tr><td>
<p>
</p><pre>cat $(find . -type f) &gt; ../collection  # WRONG
</pre>
<blockquote>
Wrong.
If any pathname contains a space, newline, or tab,
its name will be split (file “a b” will be
incorrectly parsed as two files, “a” and “b”).
If a pathname contains a globbing character like *, the shell will try to
expand it, potentially creating additional problems.
Also, if the find command matches no files,
the command will be run with no parameters;
on many commands (like cat) this will cause the program to hang on input
from standard input
(you can fix this by appending pathname /dev/null, but many
people do not know to do that).
</blockquote>
</td></tr>
<tr><td>
<p>
</p><pre>( for file in $(find . -type f) ; do  # WRONG
    cat "$file"
  done ) &gt; ../collection
</pre>
<blockquote>
Wrong, for similar reasons.
This breaks up pathnames that contain space, newline, or tab, and
it incorrectly expands pathnames if the pathnames themselves contain
characters like “*”.
</blockquote>
</td></tr>
<tr><td>
<p>
</p><pre> ( find . -type f |   # WRONG
   while read file ; do cat "$file" ; done ) &gt; ../collection
</pre>
<blockquote>
Wrong.
This works if a pathname has spaces in the middle, but it
won’t work correctly if the pathname begins or ends with whitespace
(they will get chopped off).
Also, if a pathname includes “\”, it’ll get
corrupted; in particular, if it ends in “\”, it will be
combined with the next pathname (trashing both).
In general, using “read” in shell without the “-r” option is usually
a mistake, and in many cases you should set <tt>IFS=""</tt>
just before the read.
</blockquote>
</td></tr>
<tr><td>
<p>
</p><pre>( find . -type f | xargs cat ) &gt; ../collection # WRONG, WAY WRONG
</pre>
<blockquote>
Wrong.
By default, xargs’ input is <i>parsed</i>, so
space characters (as well as newlines) separate arguments, and
the backslash, apostrophe, double-quote, <i>and</i> ampersand
characters are used for quoting.
According to the
<a href="http://www.opengroup.org/onlinepubs/009695399/">POSIX standard</a>,
you have to include the option <tt>-E ""</tt> or
underscore <i>may</i> have a special meaning too.
Note that many of the examples in the
<a href="http://www.opengroup.org/onlinepubs/009695399/">POSIX standard</a>
xargs section are wrong;
pathnames with spaces, newlines, or many other characters
will cause many of the examples to fail.
</blockquote>
</td></tr>
<tr><td>
<p>
</p><pre> ( find . -type f |
   while IFS="" read -r file ; do cat "$file" ; done ) \
          &gt; ../collection # WRONG
</pre>
<blockquote>
Wrong.
Like many programs, this assumes that you can have list of pathnames,
with one pathname per line.
But since pathnames can internally include newline,
all simple line-at-a-time processing of pathnames is wrong!
This construct is fine if pathnames
can’t include newline, but since many Unix-like systems permit,
attackers are happy to use this false assumption as an attack.
</blockquote>
</td></tr>
<tr><td>
<p>
</p><pre>cat $file
</pre>
<blockquote>
Wrong.  If $file can contain whitespace, then it could broken up and
interpreted as multiple file names, and if $file starts with dash,
then the name will be interpreted as an option.
Also, if $file contains metacharacters like “*”
they will be expanded first, producing the wrong set of filenames.
</blockquote>
</td></tr>
</tbody></table>


</p><p>
</p><h1 id="summary">Doing it correctly: A quick summary</h1>
<p>
So, how can you process pathnames <i>correctly</i> in shell?
Here’s a quick summary about how to do it correctly, for the impatient
who “just want the answer”.

</p><h2>Basic rules</h2>
<ol>
<li><a href="#doublequote">Double-quote all variable references
and command substitutions</a> unless you are
<i>certain</i> they can only contain alphanumeric characters
or you have specially prepared things
(i.e., use <tt>"$variable"</tt> instead of <tt>$variable</tt>).
In particular, you should practically always put
<tt>$@</tt> inside double-quotes;
POSIX defines this to be special
(it expands into the positional parameters as
<i>separate</i> fields even though it is inside double-quotes).
</li>
<li><a href="#ifs">Set IFS to just newline and tab</a>, if you can,
to reduce the risk of mishandling filenames with spaces.
Use newline or tab to separate options stored in a single variable.
Set IFS with
<tt>IFS="$(printf&nbsp;'\n\t')"</tt></li>
<li><a href="#prefixglobs">Prefix all pathname globs so they cannot expand to begin with
“-”</a>.
In particular, never start a glob with “?”
or “*” (such as “*.pdf”);
always prepend globs with something (like “./”)
that cannot expand to a dash.
So never use a pattern like “*.pdf”; use “./*.pdf”
instead.
</li><li><a href="#checkdash">Check if a pathname begins with “-”
when accepting pathnames</a>, and then
prepend “./” if it does.
</li><li><a href="#display-store">Be careful about
displaying or storing pathnames</a>, since they can
include newlines, tabs, terminal control escape sequences, non-UTF-8
characters (or characters not in your locale), and so on.
You can strip out control characters and non-UTF-8 characters
before display using
<tt>printf '%s' "$file" | LC_ALL=POSIX tr -d '[:cntrl:]' | iconv -cs -f UTF-8 -t UTF-8</tt>
</li><li><a href="#dashdash">Do <i>not</i> depend on
always using “<tt>--</tt>”</a>
between options and pathnames as the primary countermeasure against
filenames beginning with “-”.
You have to do it with <i>every</i> command for this to work, but
people will <i>not</i> use it consistently (they never have), and many
programs (including <tt>echo</tt>) do not support “<tt>--</tt>”.
Feel free to use “<tt>--</tt>”
between options and pathnames, but only
as an <i>additional</i> optional protective measure.
</li><li>Use a template that is known to work correctly;
below are some
<a href="http://www.dwheeler.com/encodef/evil-filenames-test">tested</a> templates.</li>
<li>Use a tool like
<a href="http://www.shellcheck.net/">shellcheck</a> to find problems
you missed.
</li></ol>



<h2>Template: <a href="#globbing">Using globs</a></h2>

<pre> # Correct portable glob use: use "for" loop, prefix glob, check for existence:
 # (remember that globs normally do NOT include files beginning with "."):
 for file in ./* ; do        # Prefix with "./*", NEVER begin with bare "*"
   if [ -e "$file" ] ; then  # Make sure it isn't an empty match
     COMMAND ... "$file" ...
   fi
 done

 # Correct portable glob use, including hidden files (beginning with "."):
 for file in ./* ./.[!.]* ./..?* ; do        # Prefix with "./*"
   if [ -e "$file" ] ; then  # Make sure it isn't an empty match
     COMMAND ... "$file" ...
   fi
 done

 # Correct glob use, simpler but requires nonstandard bash extension nullglob:
 shopt -s nullglob  # Bash extension, so globs with no matches return empty
 for file in ./* ; do        # Use "./*", NEVER bare "*"
   COMMAND ... "$file" ...
 done

 # Correct glob use, simpler but requires nonstandard bash extension nullglob;
 # you can do things on one line if you can add /dev/null as an input.
 shopt -s nullglob  # Bash extension, so globs with no matches return empty
 COMMAND ... ./* /dev/null
</pre>

<h2>Template: <a href="#find">Using find</a></h2>
<p>
The find command is great for recursively processing directories.
Typically you would specify other parameters to find
(e.g., select only normal files using “-type f”, or skip filenames
“.” and “..”), which is not shown here.
Below are the forms that always work (though some require
nonstandard extensions or fail with Cygwin),
followed by simpler ones with serious limitations.

</p><h3>Always works</h3>
<pre> # Simple find -exec; unwieldy if COMMAND is large, and creates 1 process/file:
 find . -exec COMMAND... {} \;

 # Simple find -exec with +, faster if multiple files are okay for COMMAND:
 find . -exec COMMAND... {} \+

 # Use find and xargs with \0 separators
 # (nonstandard common extensions -print0 and -0. Works on GNU, *BSDs, busybox)
 find . -print0 | xargs -0 COMMAND

 # Head-busting, but it works portably.  Use '\'' for single-quote in command.
 # Runs a subshell, so variable values are lost after each iteration:
 find . -exec sh -c '
  for file do
     ...  # Use "$file" not $file
  done' sh {} +

 # find... while loop, requires find (-print0) and shell (read -d) extensions.
 # Fails on Cygwin; in while loops, filenames ending in \r \n and \n look =.
 # Variable values may be lost unset because loop may run in a subshell.
 find . -print0 | while IFS="" read -r -d "" file ; do ...
   COMMAND "$file" # Use quoted "$file", not $file, everywhere.
 done

 # while + find with process substitution.
 # Requires nonstandard read -d (bash ok) and find -print0.
 # Rquires nonstandard process redirection &lt;(...); bash, zsh, and ksh 93
 # have process redirection, but dash and ksh 88 do not. Also,
 # the underlying system must have named pipes (FIFOs) or /dev/fd.
 # Fails on Cygwin; in while loops, filenames ending in \r \n and \n look =.
 # Variables *do* retain their value after the loop ends, and
 # you can read from stdin (change the 4s to another number if fd 4 is needed):
 while IFS="" read -r -d "" file &lt;&amp;4 ; do
   COMMAND "$file" # Use quoted "$file", not $file, everywhere.
 done 4&lt; &lt;(find . -print0)

 # Named pipe version.
 # Requires nonstandard extensions to find (-print0) and read -d (bash ok);
 # underlying system must inc. named pipes (FIFOs).
 # Fails on Cygwin; in while loops, filenames ending in \r \n and \n look =.
 # Variables *do* retain their value after the loop ends, and
 # you can read from stdin (change the 4s to something else if fd 4 needed).
 mkfifo mypipe
 find . -print0 &gt; mypipe &amp;
 while IFS="" read -r -d "" file &lt;&amp;4 ; do
   COMMAND "$file" # Use quoted "$file", not $file, everywhere.
 done 4&lt; mypipe

 # Use the <a href="http://www.dwheeler.com/encodef/index.html">author's encodef program</a>.
 # Variables *do* retain their value after the loop ends.
 # This version is POSIX portable, but you must have encodef. In practice
 # you can often use "-print0" instead of POSIX "-exec printf '%s\0' {} \;"
 for encoded_pathname in $(find . -exec printf '%s\0' {} \; | encodef ) ; do
   file="$(encodef -dY -- "$encoded_pathname")" ; file="${file%Y}"
   COMMAND "$file" # Use quoted "$file", not $file, everywhere.
 done
</pre>


<h3>Limitations</h3>
<p>
It is sometimes easier to not fully handle pathnames, especially
if you are trying to write portable shell code.
However, that code can quickly become a security vulnerability
if you use it to examine expanded archives
(such as zip or tar files), or examine a directory with files created by
another (e.g., a remote filesystem, a virtual machine controlled by
someone else or an attacker, another mobile app, etc.).
Here are examples (and their limitations):

</p><pre> # Okay if pathnames can't contain tabs or newlines; beware the assumption:
 IFS="$(printf '\n\t')"
 set -f # Needed for filenames with *, etc; <a href="#save-restore">see below on saving/restoring</a>
 for file in $(find .) ; do
   COMMAND "$file" ...
 done

 # Okay if pathnames can't contain tabs or newlines; beware the assumption:
 IFS="$(printf '\n\t')"
 set -f # Needed for filenames with *, etc; <a href="#save-restore">see below on saving/restoring</a>
 COMMAND $(find .) /dev/null

 # Okay if pathnames can't contain newlines; beware the assumption.
 # Also, this makes stdin inaccessible, and variables may not stay set.
 find . | while IFS="" read -r file ; do ...
   COMMAND "$file" # Use "$file" not $file everywhere.
 done

 # You can securely use the above approaches, even if directories have
 # evil filenames, if you can skip evil filenames.   For example, here's how to
 # skip pathnames with embedded control chars, including newline and tab:
 IFS="$(printf '\n\t')"
 controlchars="$(printf '*[\001-\037\177]*')"
 set -f # Needed for filenames with *, etc; <a href="#save-restore">see below on saving/restoring</a>
 for file in $(find . ! -name "$controlchars") ; do
   COMMAND "$file" ...
 done

 # Skip pathnames with embedded control chars, including newline and tab:
 IFS="$(printf '\n\t')"
 controlchars="$(printf '*[\001-\037\177]*')"
 set -f # Needed for filenames with *, etc; <a href="#save-restore">see below on saving/restoring</a>
 COMMAND $(find . ! -name "$controlchars") /dev/null
</pre>

<p>
</p><h2>Template: Building up a variable</h2>
<p>
There’s no easy portable way to handle multiple
arbitrary filenames in one variable and then directly use them.
Shell arrays work, but can be tricky to use in this case and are not portable.
I suggest forbidding filenames with tabs and newlines; then you
can easily use those characters as separators like this:
</p><pre> # If you build up options in a string, use tab|newline to separate filenames
 IFS="$(printf '\n\t')"
 tab="$(printf '\t')"
 command_options="-x${tab}-y"
 # If you want to put pathnames in built-up string, prevent tab|newline
 # in the pathname, use "set -f", and then you can use an unquoted variable.
 # E.g., presuming that $file doesn't contain tab|newline, -F $file is:
 command_options="${options}-F${tab}${file}"
 set -f # Needed for filenames with *, etc; <a href="#save-restore">see below on saving/restoring</a>
 mycommand $command_options "$another_pathname"
</pre>
<!-- See Bash FAQ at http://mywiki.wooledge.org/BashFAQ/024 -->


<p>
</p><h2 id="save-restore">Template: Saving and restoring “set -f”</h2>
<p>
Sometimes you need to disable file globbing in shell, especially when
receiving information from find.
POSIX includes various portable mechanisms to disable and re-enable
file globbing in shell.
The “set -f” command disables file globbing.
You can use “set -f” to disable file globbing, and
“set +f” to re-enable it.
But what if you want to use
“set -f” to disable file globbing temporarily, and later
restore whatever it was before?
One way is to put the “set -f” and what it depends on
in a subshell; that works, but then variable settings are lost once the
subshell is done.
You can also
save and restore shell option settings by doing this:
</p><pre> oldSetOptions=$(set +o)             # Save shell option settings
 ... (set -f, etc.)
 eval "$oldSetOptions" 2&gt; /dev/null  # Restore shell option settings
</pre>


<p>
</p><h1 id="basic-rationale">Rationale for the basic rules</h1>
<p>
Here is the rationale for each of the basic rules.
</p><p>
</p><h2 id="doublequote">Double-quote parameter (variable) references and command substitutions</h2>
<p>
As described by any Bourne shell programming book,
<i>always</i> use double-quotes (<tt>"</tt>)
to surround variable references and command substitutions,
unless you are certain they can only produce alphanumeric characters
or you have specially prepared things.
The dangerous characters are whitespace or shell pathname expansion (glob)
characters like “<tt>*</tt>”, because
unquoted variable references and command substitutions
undergo shell field splitting and pathname expansion:
</p><ul>
<li>Field splitting splits a word into multiple words; by default they are
split by space, tab, or newline.
This expansion can be
controlled or disabled by setting the variable <tt>IFS</tt>, but you
have to specially set it.
</li><li>Pathname expansion looks for filename patterns, and if they are found,
splits up a word into multiple words for each filename.
If the unquoted variable reference or command substitution produces
a character like <tt>*</tt>, shell will normally
try to replace that with a list of the filenames that match the pattern.
This expansion can be disabled using “<tt>set -f</tt>”.
</li></ul>
<p>
The good news is that once you get into the habit, this
is an easy style rule to follow.
Even if you know that they can only produce characters that will not
cause problems,
quoting is a good idea, since the script might change in the future.
It is easy to remember “alphanumeric characters okay” than a more
complicated rule, and if you allow more than alphanumeric characters, it
is likely that the variable will eventually allow dangerous characters.
Lots of scripts already follow this rule, so while it’s annoying,
it’s not too bad.
Here are some examples:
<table border="1">
<tbody><tr><th>Don’t use</th><th>Instead use</th></tr>
<tr><td><tt>$file</tt></td><td><tt>"$file"</tt></td></tr>
<tr><td><tt>$(pwd)</tt></td><td><tt>"$(pwd)"</tt></td></tr>
<tr><td><tt>$(dirname $file)</tt></td><td><tt>"$(dirname "$file")"</tt></td></tr>
</tbody></table>
</p><p>
By the way, it turns out that the
<a href="http://austingroupbugs.net/view.php?id=832">POSIX spec
is unclear whether or not field splitting applies to arithmetic
expansion in shell</a>; most (but not all) implementations do apply
field splitting in this case.
</p><p>
</p><p>
</p><h2 id="ifs">Set IFS to just newline and tab at the start of each script</h2>
<p>
One of the first non-comment commands in every shell script should be:
</p><pre>   IFS="$(printf '\n\t')"
   # or:
   IFS="`printf '\n\t'`"
   # Widely supported, POSIX added http://austingroupbugs.net/view.php?id=249
   IFS=$'\n\t'
</pre>
<p>
This sets the IFS variable so that
the “space” character is no longer an input field separator,
and thus only newline and tab are field separators.
If you need to run on really old systems the second form with backquotes
is better, but the first one is easier to read, POSIX compliant,
and very portable - it works on any system not in a museum.
</p><p>
This doesn’t help security very much, but it does help reliability.
If you make a mistake in your script, and the script encounters
a pathname (or other data)
with a space, your script is more likely to work correctly.
Filenames with tabs and newlines are almost never used except by attackers,
but users often use spaces; doing this will prevent file splitting
from unintentionally splitting up filenames with spaces.
So if you forget to surround a variable reference with double-quotes,
or use a for loop with a simple command substitution, it
is less likely to fail.
It is also really easy to do this; just add one line near the top.
</p><p>
The recommended IFS command sets newline and then tab.
It is harder to do it in the other order in some shells,
because $(...) consumes trailing newlines.
The easy way is to use <tt>IFS=$'\t\n'</tt>, which is widely supported but
<a href="http://austingroupbugs.net/view.php?id=249">has only recently been
added to POSIX</a>.
</p><p>
You can still build a list of command options inside a single shell variable,
even when space isn’t in IFS.
You just need to use tab or newline to separate parameters,
and <i>not</i> space.
You can even embed pathnames with spaces in this variable, since
spaces are no longer field separators.

</p><p>
You might also want to put “<tt>set -eu</tt>”
or at least “<tt>set -u</tt>”
at the beginning of your scripts, along with setting IFS.
This does nothing for pathnames, but these <i>can</i> help detect
other script errors.

</p><p>
By the way, the need for proper quoting is not limited to Bourne shells.
The
<a href="http://arstechnica.com/security/2014/10/poor-punctuation-leads-to-windows-shell-vulnerability/">Windows shell also requires proper quoting,
and improper quoting can lead to vulnerabilities</a>.
<a href="http://thesecurityfactory.be/command-injection-windows.html">A user merely needs to create filenames with characters such as ampersands</a>,
and an improperly-quoted shell program might end up running it.
For example, imagine if an attacker can create a directory of the form
“name&amp;command_to_execute”, say on a fileserver.
Then a Windows script which fails to quote properly
(e.g., it has <tt>ECHO %CD%</tt> or <tt>SET CurrentPath=%CD%</tt>
without putting double-quotes around <tt>%CD%</tt>) would end up
running the command of the attacker’s choosing.


</p><p>
</p><h2 id="prefixglobs">Prefix all globs so they cannot expand to begin with “-”</h2>
<p>
A “glob” is a pattern for pathname matching
like “*.pdf”.
Whenever you use globbing to select files,
<i>never</i> begin with a globbing character
(typically the characters “*”,
“?”, or “[”).
If you’re starting from the current directory,
prefix the glob with “./” like this:
</p><pre> cat ./*                   # Use this, NOT "cat *" ... Must have 1+ files.
 for file in ./* ; do      # Use this, NOT "for file in *" (beware empty lists)
   ...
 done
</pre>
<p>
This is important because almost all commands will interpret
a string beginning with dash as an option, not as a filename,
until they see something that does not begin with dash.
Globs are expanded by the shell into a list of filenames, and dash
is earlier in the sort order compared to before alphanumerics,
so it is easy for attackers to make this happen.
</p><p>
If you always prefix pathnames (e.g., those acquired through globs), then
pathnames starting with “-” will always be handled correctly.
Globbing is often the easiest way to handle all files, or a subset of them,
in a specific directory, but you need to make sure you do it
correctly.

</p><p>
</p><h2 id="checkdash">Check if a pathname begins with “-”
when accepting pathnames, and then prepend “./” if it does</h2>
<p>

</p><p>
Similar to the previous rule,
if you read in a pathname, as early as possible see if it begins
with “-”... if it does, prepend “./”.
This eliminates this source of pathnames that are confused as
option flags.

</p><p>
</p><h2 id="display-store">Be careful about displaying or storing pathnames</h2>
<p>

</p><p>
Filter or encode pathnames before displaying them.
The biggest problem is that
pathnames could contain control characters that control the terminal
and/or the GUI display, causing nasty side-effects on display.
Displaying pathnames can even cause a security vulnerability
in some situations (!).
If you must display pathnames, consider encoding or
stripping out control characters first (many ls implementations do this
when the output is a terminal).
You can strip out the control characters this way:
</p><pre> printf '%s' "$file" | LC_ALL=POSIX tr -d '[:cntrl:]'
</pre>

<p>
In addition, you have no way of knowing for certain
what the pathname’s character encoding is,
so if you got a pathname from someone else,
and they do not use UTF-8 (including ASCII),
you’re likely to end up with garbage
<a href="http://en.wikipedia.org/wiki/Mojibake">mojibake</a>.
</p><p>
In practice, what most people do is exchange pathnames and hope
that they are UTF-8.
If you both use the same locale, you could use that instead, but
UTF-8 is the only encoding in wide use for Unix pathnames
that can handle arbitrary languages.
Most modern GUI toolkits presume that filenames are UTF-8,
even though nothing actually ensures that this is true.
If you must display pathnames, consider forcing them to display as UTF-8.
I encourage you to <i>always</i> encode pathnames in UTF-8...
but beware that nothing actually enforces this common convention.
Thus, you will want to enforce it yourself where you can.
</p><p>
One way you can avoid displaying non-UTF-8 filenames in shell
is to try to convert them to UTF-8 using <i>iconv</i>.
The iconv program is in POSIX, and it can strip out
characters not in a given encoding.
Sadly, the encodings that must be supported by iconv are not standardized.
Still, GNU iconv supports UTF-8, and other systems are likely to do so,
so this will probably work:
</p><pre>  printf '%s' "$file" | iconv -cs -f UTF-8 -t UTF-8
</pre>

<p>
A common approach for storing pathnames in files, or to transmit them
in data formats, is to separate them with newlines and/or tabs.
Sadly, this does not work in the general case, since pathnames
can include both characters.
You need to forbid such nasty filenames, escape them, or
use \0 to separate the pathnames.
If you can forbid them, that is the easiest... but you may not
have that option.

</p><p>
</p><h2 id="dashdash">Do not depend on “--”</h2>
<p>
Many books, and the POSIX standard, mistakenly advocate
using “<tt>--</tt>” between the options and pathnames
as the primary method to deal with filenames beginning with “-”.
This is <i>impractical</i> and <i>bad advice</i>:
</p><ol>
<li>For “<tt>--</tt>” to work,
all maintainers would have to faithfully use “<tt>--</tt>”
in practically every command invocation.
That just doesn’t happen in real life,
even after decades of people trying.
People forget it all the time; no one is that consistent, especially
since code seems to work without it.
Very few commands <i>require</i> it, after all.
</li><li>You can’t do it anyway, even if you were perfectly consistent;
many programs and commands do not support “<tt>--</tt>”.
POSIX even explicitly <i>forbids</i> echo from
supporting “<tt>--</tt>”,
and echo must support “-n”
(and GNU coreutils echo supports other options too).
</li></ol>
<p>
Thus, as a practical matter you need to do something else;
by always prefixing filenames if they start with dash, as
recommended earlier, the problem disappears.

</p><p>
<i>Do</i> feel free to use “<tt>--</tt>”
between options and pathnames, when you can do it,
as an <i>additional</i> protective measure.
But using “<tt>--</tt>” as your primary (or only) mechanism
for dash-prefixed filenames is bad idea.
You are better off prefixing the pathnames when you get the pathname,
since then you only have to do it once per pathname.
Once you prefix the pathname it doesn’t matter if you remember
“<tt>--</tt>” or not; it just works correctly.

</p><p>
</p><h2>Yes, there’s more</h2>
<p>
Sadly, there’s more.
There are two major ways to get sets of pathnames in the shell,
<a href="#globbing">glob patterns</a> and the
<a href="#find">find command</a>.
Globs are primarily useful for a short list of unhidden filenames 
in one directory; find is useful for other situations, including
recursively descending into subdirectories.
</p><p>
In both cases you have to worry around what happens when there
are zero matches.
If you just gave “command” and something that gave a list of filenames,
most commands will hang while trying to read from standard input.
The easy solution in this case is to add “/dev/null” to the end...
assuming you can do that.
</p><p>
The next two sections examine
<a href="#globbing">glob patterns</a> and the
<a href="#find">find command</a> in turn.


</p><h1 id="globbing">Globbing</h1>

<p>
Globbing is a simple language specifically designed for
filename handling, primarily to create lists of unhidden files in
a particular directory.
In this language,
“*” matches all non-hidden files in the current directory,
“*.pdf” matches all non-hidden files in the current directory
ending in “.pdf” - and so on.

</p><p>
The good news about globbing in shell is that glob expansion is
built into the shell and
done <i>after</i> field (IFS) expansion.
Thus, as long as you directly use globs
as command parameters or as part of a “for” loop,
you will have no problem with
pathnames containing whitespace or control characters
(since they will not undergo field expansion).
There is also no challenge getting the information back into shell;
the shell is doing the processing.

</p><p>
However, if a pathname begins with “-”,
glob will dutifully expand it, confusing any command later.
As noted above, the recommended solution is to always prefix a glob
with something that does not begin with dash, such as
“./”.

</p><p>
Remember that globbing normally skips hidden files
(those beginning with “.”).
Often that is what you want.
If you want the hidden files in a directory instead,
you may want to use “find” instead.
You can get the hidden files with a glob by adding two more globbing
patterns:
</p><pre> .[!.]* ..?*
</pre>
<p>
In many cases even a simple glob <i>could</i> fail to match, and
adding globbing patterns to find hidden files makes this even more likely...
which leads us to the problem of handling empty pathname lists.

</p><p>
</p><h2>Beware of globs if there might be empty lists of pathnames</h2>
<p>
Beware of globbing if there might be no matches with the pattern
(and this is often the case).
By default, if a glob like <tt>./*.pdf</tt> matches no files, then
the original glob pattern will be returned instead.
</p><p>
This is almost never what you want.
E.g., in a “for” loop
this will cause the loop to execute once, but with the pattern instead of
a pathname!
Similarly, if you use a glob on a command line,
such as <tt>cat&nbsp;./*pdf</tt>, the result will be a request to open
a non-existent file... which is almost never what you want.
</p><p>
You can use use globbing in a <tt>for</tt> loop, even if it might not
match anything, using one of two approaches.
One approach, which is completely portable,
is to re-test for the existance of the file before using it in the loop:
</p><pre> for file in ./* ; do        # Use this, NOT "for file in *"
   if [ -e "$file" ] ; then  # Make sure it exists and isn't an empty match
     COMMAND ... "$file" ...
   fi
 done
</pre>
<p>
This is both ugly and a little inefficient (you have to re-test each file
again).
There are also pathological cases where
the pattern doesn’t match but there
is a file that is identical to the unmatched pattern
(though for typical patterns that can’t happen), so you have to check
your pattern to see if that could happen.
</p><p>
A more efficient but nonstandard solution for empty matches is to use
a nonstandard shell extension called “null globbing”.
Null globbing fixes this by replacing an unmatched pattern with nothing at all.
In bash you can enable nullglob with
“<tt>shopt -s nullglob</tt>”.
In zsh, you can use <tt>setopt NULL_GLOB</tt> for the same result.
Then this will work correctly:
</p><pre> shopt -s nullglob  # Bash extension, so that empty glob matches will work
 for file in ./* ; do        # Use this, NOT "for file in *"
     COMMAND ... "$file" ...
 done
</pre>
<p>
Null globbing can work well on the command line too, but there’s
a catch.
If all patterns might be empty, you have to include at least one file
(such as /dev/null) that is okay to include, <i>or</i> it needs
to be okay to run the command without any pathname arguments.
Thus, you can use “cat&nbsp;./*.pdf&nbsp;/dev/null”.
</p><p>
Another problem with globbing is that if the list of matches is too
long, on some older shells it will <i>also</i> fail.
In short, in robust scripts, globbing should normally be used only as
a “for” loop’s list.
</p><p>
</p><h2>The globstar extension</h2>
<p>
Traditional globbing is only useful when you want to process files in
a particular directory.
Some shells have added a nonstandard
“globstar” extension, but it’s both nonstandard and has various limitations.
I discuss it here, but you probably want to use <tt>find</tt> (discussed next).
</p><p>
With the globstar extension, the pattern “**”
returns every pathname (including directories) in the current directory,
recursively;
it omits dot files, doesn’t descend into dot dirs, and
sorts the file list.
</p><p>
Bash version 4 recently added this, but you must enable it with
“<tt>shopt -s globstar</tt>”.
The <a href="http://www.mail-archive.com/bug-bash@gnu.org/msg04858.html">
zsh shell originally came up with this</a>, and ksh93 was the first to copy it
(but in ksh you have to enable it with “set -G”).
Note that there’s no standard way to invoke it!
</p><p>
If you use this in a <tt>for</tt> loop list and combine it with nullglob,
you can handle absolutely all pathnames easily and efficiently,
including the empty case.
That sounds great, but watch the fine print... I think there are many
reasons to avoid this right now.
It’s nonstandard, and gives you little control over the recursion.
Most importantly, at least some
implementations have trouble if there are links in the directories.
<a href="http://en.chys.info/2009/04/globstar-in-bash-4-follows-symlinks/">
Bash 4, at least, can get stuck in infinite loops if there are links</a>.
In many cases, <tt>find</tt> is currently the better approach for reliably
doing recursive descent into directories.

</p><p>
</p><h1 id="find">Find</h1>
<p>
If you want to process files beyond what normal globbing can do
(e.g., recursively handle directories), or you don’t like the
limitations on having to re-check for non-matches, use <tt>find</tt>.
Re-implementing accurate directory traversal in the shell is possible,
but both painful and silly;
you would have to deal with symbolic links, hard links,
renames during traversal, and other problems.
The <tt>find</tt> tool is designed to handle this job; let it do its job.
Sometimes you want to retrieve pathnames from programs other than find;
in those cases the issues tend to be very similar.

</p><p>
An advantage of <tt>find</tt> is that
it has lots of options for controlling how you process files and directories.
You can use options to limit it to one directory, determine the ordering,
and so on.
It normally processes all files (including hidden ones), but you can use
this pattern to skip hidden files (omit the “?” to skip directory “.”):
</p><pre> find . -name '.?*' -prune -o ....
</pre>

<p>
The <tt>find</tt> command is always passed a starting directory,
and it always returns values beginning with that directory.
Thus, as long as
the starting directory doesn’t begin with “-”
you won’t have a problem with leading “-”.

</p><p>
If you can directly use the find “-exec” option to run the command you
want to use with the file, that is the easy way.
Sadly, this is awkward to do in many cases, so you often want to
get pathames from find back into the shell.

</p><p>
A challenge with find (and any other external program)
is that it is more complicated to
portably get information back into the shell.
Pathnames can embed newlines; that means that
reading filenames line-by-line fails (e.g, by read),
splitting using IFS and newline cannot work directly, and
command command substitution $(...) is awkward because it
strips away trailing newlines.
There are additional problems with Cygwin; Cygwin sometimes
silently maps ending \r\n into \n, but it is legal to have filenames
that differ only because one ends in \r\n and the other ends in \n;
As a result, some constructs for handling arbitrary
filenames do not work on Cygwin.
Pathnames can also embed tabs and spaces, which by default causes
unwanted field splitting.
The usual approach is to
<a href="#null">separate pathnames with \0</a>, but the mechanisms to
handle this are not in the POSIX standard
(e.g., options for “read” or “xargs” to handle such things).

</p><p>
Some of these approaches use “read”, which is tricky to use.
You typically need to use the “-r” option
(so backslash is considered part of the line, and not an escape mechanism;
otherwise filenames containing backslashes will cause problems).
You also typically have to set IFS to be empty for the read command;
otherwise, a pathname that
includes IFS characters at the end of a filename would be corrupted
(see the POSIX.1-2008 specification lines 103920-103925).
If you want to read pathnames separated by \0 you’ll need another option,
the <a href="#null">discussion on null-separated pathnames</a>
gives more detail.

</p><p>
You’d like to use simple “for” loops,
but by default if there is
a pathname (glob) expansion character like “*” in a pathname,
find will return the “*”.
If the shell receives a glob expansion character from a command
substitution, by default the character will be <i>re-expanded</i> by the shell.
You cannot just quote that expansion, either; that would
make it appear that the list is just one pathname.
Thus, in many cases you need to use “set -f” to disable
expansion of filenames.

</p><p>
You could combine xargs with find using a pipe,
and use newlines to separate pathnames, but <b>don’t do it</b>.
The problem is that xargs interprets many characters in surprising ways, so
it’s hard to use xargs correctly when using newlines as
separators.
The correct <i>portable</i> way to use xargs with newline separators
requires that you pipe pathnames through
another command like <tt>sed</tt> to do character substitutions.
The result is complicated, hard to read, rediculously inefficient, and
isn’t better than many other alternatives
(e.g., it doesn’t handle newlines either); here it is:
</p><pre> find . | sed -e 's/[^A-Za-z0-9]/\\&amp;/g' | xargs -E "" COMMAND # DO NOT DO
</pre>


<p>
</p><h1 id="null">Using null-separated pathnames</h1>
<p>
If you want to exchange pathnames (in their full generality)
between programs,
or store them for later,
a common solution is to use byte 0 (aka \0 or null)
to separate pathnames.
This works because pathnames, by definition, cannot include byte 0.
</p><p>
This is very useful, and it works nicely, but note that
there are downsides to this approach:
</p><ul>
<li>The easy way to use this convention requires
the use of nonstandard extensions that are not a part of the
<a href="http://www.opengroup.org/onlinepubs/009695399/">POSIX standard</a>,
so the result is non-standard and less portable.
Still, many toolsets have a few tool extensions to support this,
including the GNU, *BSD, and busybox toolsets.
Hopefully someday POSIX will add this.
</li><li>The option names to use this convention today (when available)
are jarringly inconsistent.
In particular, <tt>perl</tt> uses <tt>-0</tt>,
while the GNU tools options are as follows:
<tt>sort -z</tt>, <tt>find -print0</tt>, <tt>xargs -0</tt>,
and <tt>grep</tt> accepts either <tt>-Z</tt> or <tt>--null</tt>.
The equivalent option in the bash shell <tt>read</tt> command
is <tt>-d ""</tt> (aka empty delimeter).
</li><li>This convention is supported by only a few tools
(e.g., the GNU toolset includes support, but in only a few of its tools).
</li><li>This format is more difficult to view and modify,
in part because so few tools support it, compared to the
line-at-a-time format that is widely supported.
</li><li>Most shells cannot store byte 0 in a variable at all.
You can’t even pass such null-separated
lists back to the shell via command substitution;
<tt>cat&nbsp;$(find&nbsp;.&nbsp;-print0)</tt> and
similar “for” loops don’t work.
Even the POSIX standard’s version of
“read” can’t use \0 as the separator
(POSIX’s read has the -r option, but not bash’s -d option).
</li></ul>
<p>
But if you want maximum generality when recursing into subdirectories,
this is a common and relatively painless way to do it.

</p><p>
</p><h1 id="encoding">Encoding pathnames</h1>
<p>
</p><p>
It <b>is</b> possible to encode pathnames
so that all pathnames can be handled.
There is no standard POSIX mechanism for doing this encoding, unfortunately.
</p><p>
<a href="http://www.dwheeler.com/essays/encodef/index.html">encodef</a> is a small utility
I wrote that can encode and decode filenames in a few formats.
With it, you can do this:

</p><pre> # This version is POSIX portable; in practice
 # you can often use "-print0" instead of "-exec printf '%s\0' {} \;"
 for encoded_pathname in $(find . -exec printf '%s\0' {} \; | encodef ) ; do
   file="$(encodef -d -Y -- "$encoded_pathname")" ; file="${file%Y}"
   COMMAND "$file" # Use quoted "$file", not $file, everywhere.
 done
</pre>


<!--
Unfortunately, there is no single standard encoding for pathnames
that doesn&#8217;t use null bytes; instead,
there are many similar yet incompatible encodings.
What&#8217;s worse, utilities often do not support any of them well.

But, let&#8217;s take a look.
<p>
<h2>printf(1) %b (pfb) encoding</h2>
<p>
One approach is to use &#8220;printf %b&#8221; encoding, pfb encoding for short.
This is the encoding supported by the printf(1) &#8220;%b&#8221; format, and
it has several advantages:
printf is part of the POSIX 2008 specification, it is widely implemented,
and it is typically a shell builtin (making it speedier).
This encoding uses the backslash (&#8216;\&#8217;) to introduce an escape sequence,
and the POSIX specification supports
&#8216;\\&#8217; (for backslash itself), &#8216;\a&#8217; , &#8216;\b&#8217; , &#8216;\f&#8217; , &#8216;\n&#8217; , &#8216;\r&#8217; , &#8216;\t&#8217; , &#8216;\v&#8217;,
and
&#8216;\ddd&#8217; (where ddd is a 1-3 digit octal number).
This is really easy to read, because it&#8217;s similar to other formats.
As long as you escape all characters ranged 1-31,
you can easily include the encoded pathname in other tab-delimited,
newline-per-record files.
(If you&#8217;re on an EBCDIC system (!), you can still escape \n, but in
EBCDIC the \n is outside the range of 1-31.)
There&#8217;s a gotcha in shell, though; pathnames can end in newline, and
a newline at the end of a command substitution is removed.
So, this would be a wrong way to decode it:
<pre>
 # WRONG: this encoding fails if encoded_pathname ends in \n
 cat encoded_pathnames |
  while IFS="" read -r encoded_pathname ; do
    file="$(printf "%b" "$encoded_pathname")"
    ...
  done
</pre>
<p>
Instead, you need to decode such filenames this way in shell, using
some character like X to protect any terminating newline:
<pre>
 # CORRECT: Unencodes all possible filenames.
 cat pfb_encoded_pathnames |
  while IFS="" read -r encoded_pathname ; do
    file="$(printf "%bX" "$encoded_pathname")" ; file="${file%X}"
    ...
  done
</pre>
<p>
But what about <b>generating</b> or <b>encoding</b> pathnames into pfb format?
Unfortunately, the POSIX standard does not include an easy
option for <b>find</b> or other tools to generate this format.
The easiest thing to do this is to use the widely-implemented -print0 option
with find, then filter it through something to convert to this format.
Here&#8217;s a small C program I wrote,
<a href="http://www.dwheeler.com/misc/nul2pfb.c">nul2pfb.c</a>,
which converts pathname lists ending in \0 to line-oriented lists
using the pfb escapes.
There are several different ways to encode into pfb;
I recommend encoding newline as \n and tab as \t, to make them obvious,
and encoding space as \040 (so you don&#8217;t need to worry about unintentional
splitting).
You really need to encode all characters less than or equal to 32, and
those greater than or equal to 127.
Yes, that makes international characters harder to read, but it preserves
them, and that&#8217;s more important.
Since you cannot be sure of the character encoding used in pathnames,
and there&#8217;s no guarantee that a pathname obeys the rules anyway,
it&#8217;s safest to encode everything.
And since we&#8217;re going that far, we may as well encode the shell
metacharacters &#8220;*&#8221;, &#8220;?&#8221;, &#8220;[&#8220;, and &#8220;!&#8221;, so that if we forget to surround
variable references with double-quotes we&#8217;ll be okay.
In the end, I decided to encode <b>every</b> character except for
alphanumerics, &#8220;/&#8221;, &#8220;.&#8221;, &#8220;_&#8221;, and &#8220;:&#8221;, since that gives maximum safety
while still making most ordinary system pathnames readable.
(This encodes &#8220;-&#8221;, which is good because a leading &#8220;-&#8221; can cause problems).
A nice side-effect of encoding so many characters is that if you
forget to decode them, you&#8217;ll probably detect that fairly early in testing.
It&#8217;d be nice if <tt>find</tt> and <tt>ls</tt>
could generate pfb encoding directly, and if <tt>xargs</tt> could directly
process them, but at least you can use null byte encoding to get there.
-->
<!--
perl -n0 -e 'print;'
-->
<!--
<p>
GNU ls includes some nonstandard quoting options, but none of them
(at this time) quite output the pfb format.
You would think its &#8220;-b&#8221; (aka - -escape or
- -quoting-style=escape) would be the same thing, but it&#8217;s not;
this option changes space to <tt>'\&nbsp;'</tt>, and pfb won&#8217;t accept that.
GNU ls&#8217; -Q (- -quote-name or - -quoting-style=c) encloses pathnames in
double-quotes, but even when you strip out the external double-quotes,
it generates <tt>'\"'</tt> for double-quotes which also isn&#8217;t in pfb.
At this time it&#8217;s easier to use find,
and then use depth control if you just want one level.
<p>
Anyway, this brings us a final approach to handling pathnames in shell.
This one lets us use a <tt>for</tt> loop, which is very nice, because
we now keep all file descriptors (including stdin) and variable setting
works as expected.
The encoding and decoding we have to do is unfortunate, but there it is:
<pre>
 # CORRECT, requires find -print0 and author's nul2pfb program.
 for encoded_pathname in $(find . -print0 | nul2pfb) ; do
   file="$(printf "%bX" "$encoded_pathname")" ; file="${file%X}"
   # Use "$file" from here on...
 done
</pre>
-->


<!--
Todo: Look at "find".
-ls, -fls, -printf

Look at "xargs".
-->

<!--
<p>
<h2>Other encodings</h2>
<p>
There are many other possible encodings.
GNU ls has several ways to encode pathnames; its &#8220;-b&#8221; format is
quite reasonable, and is similar to pfb,
but note that it puts \ in front of space, so it&#8217;s a little more trouble to
decode.
<p>
Another option is
<a href="http://en.wikipedia.org/wiki/Percent-encoding">percent encoding</a>,
aka URL encoding.
Basically, any byte can represented by &#8216;%&#8217; followed by a 2-digit hexadecimal
number.
<p>
The default format of xargs is a stinking, rotting mess,
and I do not recommend it at all.
It&#8217;s not compatible with anything, including the <b>find</b> command
it&#8217;s supposed to work with.
You can double-quote or single-quote text, but they have to end before
the end of a line.
The only way to encode newline is to precede it with &#8220;\&#8221;,
outside of a double-quote or single, which makes it hideously hard
to deal with.
You&#8217;re better off using the widely-supported -0 option, even if
it is technically non-portable.
If you want to use encodings, it&#8217;d be nice if xargs supported pfb encoding,
or if there was some other widely-supported encoding.
-->

<h1>A quick aside about newline</h1>

Newline can be a little tricky to get into a shell variable.
You can’t do:
<pre>  newline="$(printf '\n')"
</pre>
Because after the $(...) command is executed, any trailing newline is
removed.
<p>
One alternative is:
</p><pre>newline='
'
</pre>
But this can get corrupted by programs that change the
encoding of file end-of-lines.
<p>
The following is a standards-compliant trick to get newline into a variable:
</p><pre>newline="$(printf '\nX')"
newline="${newline%X}"
</pre>

<p>
That is a pain, obviously.
More recently,
<a href="http://austingroupbugs.net/view.php?id=249">
POSIX added support for <tt>$'...'</tt></a>.
Most shells, though not all, already support it.
On a shell that does, you can do this:
</p><pre>newline=$'\n'
</pre>


<p>
</p><h1>Could the POSIX standard be changed to make file processing easier?</h1>
<p>
The POSIX standard could (and should!) be modified to make it easier to
to handle the outrageously permissive pathnames that are permitted today.
Basically, we need extensions to make globbing and find easier to use.
</p><p>
</p><h2>Globbing</h2>
<p>
There are two basic problems with globbing:
</p><ol>
<li>
Globbing in shell returns junk (the pattern) when there are zero matches.
There should be a shell option (typically called a “nullglob” option) so
an empty list is returned if nothing matches and there was at least
one metacharacter.
Oddly enough, the underlying glob() function has an option
that’s close to this, but there’s no standard way for shells
to take advantage of it!
Bash, ksh, and others have support, but not in a common
standard way, and glob() doesn’t support exactly what is needed either
(<a href="http://austingroupbugs.net/view.php?id=247">bugid:247</a>).
</li><li>
Globbing normally replies pathnames beginning with “-”.
There should be an option that when set prepends “./” to any glob result that
begins with “-”.
This should be an option for glob(), as well as for the shell.
I think the standard should also state that implementations <i>may</i>
enable this by default.
Not all real-world commands support “--”, and users often forget to add it;
we need to have a mechanism to automatically deal with pathnames
beginning with “./” if you need them.
</li></ol>

<p>
</p><h2>Find / null separators</h2>
<p>
There also needs to be standard way to use <tt>find</tt> with
arbitrary pathnames.
The normal way to handle this is by separating pathnames with the null (\0)
character; a few changes would simplify this:
</p><ol>
<li>Extend existing commands to generate or use null-separated pathname lists.
At the least, add “find -print0”
(<a href="http://austingroupbugs.net/view.php?id=243">bugid:243</a>)
and “xargs -0” (<a href="http://austingroupbugs.net/view.php?id=244">bugid:244</a>)
since these are already widely implemented.
For consistency, I think “-0” should be the standard option name for
null-separated lists.
It’d be useful to add “grep -0”
(GNU grep accepts either -Z or --null)
and “sort -0” (GNU sort uses -z).
Once added, this common template would be portable:
<pre> find . -print0 | xargs -0 COMMAND
</pre>
</li><li>
Extend the shell’s <tt>read</tt> so that it can easily read
null-separated streams.
(<a href="http://austingroupbugs.net/view.php?id=245">bugid:245</a>)
Bash can do this today, but it’s painful; the command is
<tt>IFS="" read -d "" -r</tt> which is overly complicated
I believe there should be a new “-0” option for read, which says
“ignore IFS, and just read until the next \0 byte”
(<a href="http://www.dwheeler.com/misc/read0.patch">Here’s a bash 4.1 patch</a>).
You can then do this
(which makes it easier to have long command sequences, as long as you
don’t need stdin):
<pre> find . -print0 | while read -0 file ; do ... done
</pre>

</li><li>
Extend the shell so that its <tt>for</tt> loop can handle a
null-separated list.
This one is harder; it’s not obvious how to do this.
My current theory is that there be a new shell option ‘nullseparator”;
when enabled, IFS is ignored, and instead \0 is the input seperator.
Then, extend the shell’s <tt>for</tt> loop syntax so that if you say
<tt>then in</tt> instead of <tt>in</tt>, this mode is temporarily
enabled while the list is processed (the original setting is then restored).
(I originally had <tt>null in</tt>, but using <tt>then in</tt>
means that no new keywords are needed.)
You could then do this:
<pre> for file then in $(find . -print0) do ... done
</pre>
</li></ol>
<p>
As a side note, it’d be nice if the <tt>$'...'</tt> construct was
standard, as it makes certain things easier
(<a href="http://austingroupbugs.net/view.php?id=249">bugid:249</a>).


</p><h1>If pathnames were limited, would it be better?</h1>

<p>
Shell programming is remarkably easy in many cases;
what’s sad is that this common case (file processing)
is far complicated than it needs to be.
This is <i>not</i> a problem limited to shell; while shell is especially
tricky, it is difficult to correctly process POSIX pathnames
in <i>all</i> languages.
</p><p>
Fundamentally, the rules on pathnames are too permissive.
Extending POSIX would make it somewhat easier, and we should do that.
However,
It would be <i>much</i> simpler if systems imposed a few simple
rules on pathnames, such as prohibiting control characters
(<a href="http://austingroupbugs.net/view.php?id=251">bugid:251</a>),
prohibiting leading “-”, and requiring pathnames to be UTF-8.
Then you could always print pathnames safely, and these
“normal” shell constructs would always work:
</p><pre> # This works if pathnames never begin with "-" and nullglob is enabled:
 for file in *.pdf ; do ... done           # Use "$file" not $file
 # This works if pathnames have no control chars and IFS is tab and newline:
 set -f
 for file in $(find .) ; do ... done        # Use "$file" not $file
</pre>
<p>
A good general principle in security is that you should whitelist input,
and only accept patterns that pass the whitelist.
However, currenly most kernels have <i>no</i> mechanism for whitelisting
file creation; they just create whatever garbage comes to mind.
Since they accept essentially anything, it becomes much harder to
guard against the data later.
</p><p>
I think that we should <i>both</i> extend the POSIX standard <i>and</i>
limit the permitted pathnames.
Not all systems will limit pathnames, so we need standard mechanism for them.
But the new standard mechanisms simply can’t be as simple as restricting
pathnames; restricting pathnames makes systems far easier
to use correctly.
<!--
The POSIX standard already notes that the worst pathnames are not portable.
I hope that some systems simply forbid control characters and
leading "-"
I don't know if standards and kernel implementors will be willing to
outright forbid filenames with control characters or leading "-", though.
If not, I think there are ways to modify the standards and implementations
to ease correct pathname processing if these other names continue to
be permitted.
-->
</p><p>
Please see my
<a href="http://www.dwheeler.com/essays/fixing-unix-linux-filenames.html">
paper on fixing Unix/Linux filenames</a>
for more about this.
</p><p>
I’ve also done some work on how to encode/decode pathnames/filenames; see the
<a href="http://www.dwheeler.com/encodef">encodef home page</a> for
more information.
</p><p>
But for now, this is how to handle pathnames properly in
shell programs.


</p><p>
</p><hr>
<p>
Feel free to see my home page at
<a href="http://www.dwheeler.com/">http://www.dwheeler.com</a>.
You may also want to look at my paper
<a href="http://www.dwheeler.com/oss_fs_why.html">Why OSS/FS? Look at
the Numbers!</a> and my book on
<a href="http://www.dwheeler.com/secure-programs">how to develop
secure programs</a>.
And, of course, my
<a href="http://www.dwheeler.com/essays/fixing-unix-linux-filenames.html">
paper on fixing Unix/Linux/POSIX filenames</a>.

</p><p>
(C) Copyright 2010-2013 David A. Wheeler.
Released under Creative Commons CC-BY-SA (any version), GNU GPL v2+,
and the
<a href="http://www.opencontent.org/openpub/">
Open Publication License (version 1.0 or later)</a>.
You can use this under any of those licenses; if you do not say otherwise,
then you release it under all of them.
In addition, Mendel Cooper has explicit authorization to include this
(or any modified portion) as part of his
“Advanced Bash Scripting Guide”.
Let me know if you need other exceptions; my goal is to get this information
out to the world!





</p></body><!--
Note: There is no portable way to store truly arbitrary multiple pathnames in a
single shell variable, because a pathname might include awkward sequences such
as newline, shell has no portable general array operation, and
shells have trouble embedding the NUL character
(see below for options on how to deal with this).
<p>
Here is an example of how to build up command options in multiple lines,
presuming that you are not embedding pathnames inside the command options
(notice that <tt>$options</tt> is not quoted):
<pre>
  tab="$(printf '\t')"     # Use tab as separator
  command_options="-x{tab}-y${tab}option_value"
  command_options="${command_options}${tab}-z"  # Build up the options.
  COMMAND $command_options "$another_pathname"
</pre>
<p>
Now, what if you want to write a portable script that
builds up a single variable with command options, <i>and</i>
you want to embed pathnames in it?
You can use the positional parameters, but this turns out to be tricky.
You can use array variables (bash has them), but these are not portable.
<p>
If you want to build up a string variable with multiple options
and possibly pathnames, which is a common pattern,
I think you're better off giving up trying to
support filenames with newlines and tabs.
It's just too hard for a crazy case.
Instead, error out or warn and ignore filenames with newlines and tabs,
ensure that IFS is set to just newline and tab,
and use tab or newline as the separator inside the variable.
As always, make sure that any filename beginning with dash
is prefixed, e.g., with "./".
Then just before you run COMMAND with unquoted options,
use <tt>set&nbsp;-f</tt> to disable pathname expansion.
The <tt>set&nbsp;-f</tt> is key;
otherwise filenames with pathname expanders in them, like *,
will be misinterpreted as requiring expansion.
The only thing that happens after expanding the fields (e.g., with tabs)
is pathname expansion, so by turning it off you're safe.
Here is a sample:

<pre>
  IFS="$(printf '\n\t')"
  tab="$(printf '\t')"
  command_options="-x${tab}-y"
  # Presuming that $file doesn't contain tab|newline, -F $file is:
  command_options="${command_options}-F${tab}${file}"
  oldSetOptions=$(set +o)   # Backup shell option settings
  set -f # Do not expand *, etc., when using $command_options below
  mycommand $command_options "$another_pathname"
  eval "$oldSetOptions" 2&gt; /dev/null  # Restore shell option settings
</pre>

--></html>